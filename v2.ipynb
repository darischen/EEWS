{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and Configure GPU Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available\n",
      "TensorFlow Version:  2.10.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3111166822158018462\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Check if GPU is available and set memory growth\n",
    "gpus = len(tf.test.gpu_device_name())\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "# Print TensorFlow version and device details\n",
    "print(\"TensorFlow Version: \", tf.__version__)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DataFrames and PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the combined data:\n",
      "         Date       Open       High        Low      Close  Adj Close  \\\n",
      "0  1999-11-18  32.546494  35.765381  28.612303  31.473534  27.068665   \n",
      "1  1999-11-19  30.713520  30.758226  28.478184  28.880543  24.838577   \n",
      "2  1999-11-22  29.551144  31.473534  28.657009  31.473534  27.068665   \n",
      "3  1999-11-23  30.400572  31.205294  28.612303  28.612303  24.607880   \n",
      "4  1999-11-24  28.701717  29.998211  28.612303  29.372318  25.261524   \n",
      "\n",
      "       Volume  \n",
      "0  62546300.0  \n",
      "1  15234100.0  \n",
      "2   6577800.0  \n",
      "3   5975600.0  \n",
      "4   4843200.0  \n",
      "Checking for missing values in each column:\n",
      "Date           0\n",
      "Open         683\n",
      "High         683\n",
      "Low          683\n",
      "Close        683\n",
      "Adj Close    683\n",
      "Volume       683\n",
      "dtype: int64\n",
      "Shape of the cleaned data: (28147685, 7)\n",
      "First few rows of the cleaned data:\n",
      "         Date       Open       High        Low      Close  Adj Close  \\\n",
      "0  1999-11-18  32.546494  35.765381  28.612303  31.473534  27.068665   \n",
      "1  1999-11-19  30.713520  30.758226  28.478184  28.880543  24.838577   \n",
      "2  1999-11-22  29.551144  31.473534  28.657009  31.473534  27.068665   \n",
      "3  1999-11-23  30.400572  31.205294  28.612303  28.612303  24.607880   \n",
      "4  1999-11-24  28.701717  29.998211  28.612303  29.372318  25.261524   \n",
      "\n",
      "       Volume  \n",
      "0  62546300.0  \n",
      "1  15234100.0  \n",
      "2   6577800.0  \n",
      "3   5975600.0  \n",
      "4   4843200.0  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load all CSV files from \"etfs\" and \"stocks\" directories\n",
    "data_dir_etfs = './data/etfs'\n",
    "data_dir_stocks = './data/stocks'\n",
    "\n",
    "all_files = []\n",
    "for directory in [data_dir_stocks, data_dir_etfs]:\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            all_files.append(os.path.join(directory, file))\n",
    "\n",
    "# Concatenate all data into a single DataFrame\n",
    "data_list = []\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file)\n",
    "    data_list.append(df)\n",
    "\n",
    "data = pd.concat(data_list)\n",
    "\n",
    "# Display the first few rows of the concatenated DataFrame\n",
    "print(\"First few rows of the combined data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Checking for missing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Display the shape of the cleaned data\n",
    "print(\"Shape of the cleaned data:\", data.shape)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "print(\"First few rows of the cleaned data:\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaN values in data: False\n",
      "Checking for infinity values in data: False\n",
      "Shapes of datasets:\n",
      "X_train: (22518138, 10, 6)\n",
      "y_train: (22518138, 6)\n",
      "X_test: (5629527, 10, 6)\n",
      "y_test: (5629527, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert 'Date' column to datetime and set as index\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Check for NaN and infinity values\n",
    "print(\"Checking for NaN values in data:\", np.isnan(scaled_data).any())\n",
    "print(\"Checking for infinity values in data:\", np.isinf(scaled_data).any())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "# Prepare the input and output for the model\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        X.append(dataset[i:(i + look_back), :])\n",
    "        y.append(dataset[i + look_back, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 10\n",
    "X_train, y_train = create_dataset(train_data, look_back)\n",
    "X_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "# Display shapes of the datasets\n",
    "print(\"Shapes of datasets:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "703692/703692 [==============================] - 7905s 11ms/step - loss: 5.9996e-06 - val_loss: 8.5124e-07\n",
      "Epoch 2/5\n",
      "703692/703692 [==============================] - 7691s 11ms/step - loss: 5.6159e-07 - val_loss: 4.9473e-09\n",
      "Epoch 3/5\n",
      "703692/703692 [==============================] - 7637s 11ms/step - loss: 4.8123e-07 - val_loss: 7.1862e-09\n",
      "Epoch 4/5\n",
      "703692/703692 [==============================] - 7646s 11ms/step - loss: 4.1262e-07 - val_loss: 3.7192e-08\n",
      "Epoch 5/5\n",
      "703692/703692 [==============================] - 7714s 11ms/step - loss: 3.9674e-07 - val_loss: 6.6723e-09\n",
      "175923/175923 [==============================] - 689s 4ms/step - loss: 6.6723e-09\n",
      "Test Loss: 6.6722782854355955e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mv2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mv2\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=150, return_sequences=True, input_shape=(look_back, X_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(units=150, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=X_train.shape[2]))  # Output layer has same number of units as the number of features\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "model.save('mv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:11: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 1.21.3)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_tensor_or_tensor_list' from 'keras.utils.tf_utils' (c:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\utils\\tf_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the NVDA CSV file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nvda_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/stocks/nvda.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf. namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], _module_wrapper\u001b[38;5;241m.\u001b[39mTFModuleWrapper):\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\api\\keras\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\api\\keras\\__internal__\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.keras.__internal__ namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\api\\keras\\__internal__\\layers\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf.keras.__internal__.layers namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRandomLayer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_preprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageAugmentationLayer\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n",
      "File \u001b[1;32mc:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\engine\\base_layer.py:54\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# A module that only depends on `keras.layers` import these from here.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_snake_case  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_tensor_or_tensor_list  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_format\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_tensor_or_tensor_list' from 'keras.utils.tf_utils' (c:\\Users\\daris\\anaconda3\\envs\\py39\\lib\\site-packages\\keras\\utils\\tf_utils.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the NVDA CSV file\n",
    "nvda_df = pd.read_csv('./data/stocks/nvda.csv')\n",
    "\n",
    "# Ensure the 'Date' column is a datetime and set as index\n",
    "nvda_df['Date'] = pd.to_datetime(nvda_df['Date'])\n",
    "nvda_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_nvda = scaler.fit_transform(nvda_df)\n",
    "\n",
    "# Define look_back period\n",
    "look_back = 60\n",
    "\n",
    "# Function to create dataset\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset) - look_back):\n",
    "        a = dataset[i:(i + look_back), :]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, :])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Check for NaN and infinity values\n",
    "print(\"Checking for NaN values in data:\", np.isnan(scaled_nvda).any())\n",
    "print(\"Checking for infinity values in data:\", np.isinf(scaled_nvda).any())\n",
    "\n",
    "# Create dataset for NVDA\n",
    "X_nvda, y_nvda = create_dataset(scaled_nvda, look_back)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = load_model('mv2')\n",
    "\n",
    "# Make predictions for NVDA\n",
    "nvda_predictions = model.predict(X_nvda)\n",
    "\n",
    "# Inverse transform the predictions and the actual values\n",
    "nvda_predictions_full = np.zeros_like(scaled_nvda)\n",
    "nvda_actual_full = np.zeros_like(scaled_nvda)\n",
    "\n",
    "nvda_predictions_full[look_back:look_back+len(nvda_predictions), -nvda_predictions.shape[1]:] = nvda_predictions\n",
    "nvda_actual_full[look_back:look_back+len(y_nvda), -y_nvda.shape[1]:] = y_nvda\n",
    "\n",
    "# Perform inverse transformation\n",
    "nvda_predictions = scaler.inverse_transform(nvda_predictions_full)[look_back:look_back+len(nvda_predictions), -nvda_predictions.shape[1]:]\n",
    "nvda_actual = scaler.inverse_transform(nvda_actual_full)[look_back:look_back+len(y_nvda), -y_nvda.shape[1]:]\n",
    "\n",
    "# Define a function for rolling predictions into the future\n",
    "def predict_future(data, model, look_back, future_steps):\n",
    "    future_predictions = []\n",
    "    current_input = data[-look_back:]\n",
    "\n",
    "    for _ in range(future_steps):\n",
    "        current_input_reshaped = current_input.reshape((1, look_back, current_input.shape[1]))\n",
    "        next_prediction = model.predict(current_input_reshaped)\n",
    "        future_predictions.append(next_prediction)\n",
    "        current_input = np.vstack([current_input[1:], next_prediction])\n",
    "\n",
    "    return np.array(future_predictions).reshape((future_steps, -1))\n",
    "\n",
    "# Number of future steps to predict\n",
    "future_steps = 60  # Adjust this to your needs\n",
    "\n",
    "# Make future predictions\n",
    "future_predictions = predict_future(scaled_nvda, model, look_back, future_steps)\n",
    "\n",
    "# Inverse transform future predictions\n",
    "future_predictions_full = np.zeros((future_steps, scaled_nvda.shape[1]))\n",
    "future_predictions_full[:, -future_predictions.shape[1]:] = future_predictions\n",
    "future_predictions = scaler.inverse_transform(future_predictions_full)[:, -future_predictions.shape[1]:]\n",
    "\n",
    "# Plot the actual vs predicted values for NVDA\n",
    "features = ['Close']  # Adjust this if your CSV has different column names\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(len(features), 1, i+1)\n",
    "    plt.plot(nvda_actual[:, i], color='blue', label=f'Actual NVDA {feature}')\n",
    "    plt.plot(nvda_predictions[:, i], color='red', label=f'Predicted NVDA {feature}')\n",
    "    plt.plot(np.arange(len(nvda_actual), len(nvda_actual) + future_steps), future_predictions[:, i], color='green', label=f'Future Predicted NVDA {feature}')\n",
    "    plt.title(f'NVDA {feature} Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(f'{feature} ($)')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
